{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Logistic Regression**"
      ],
      "metadata": {
        "id": "75la8ojHcld6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "K1Ox_LV0c9HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.1 What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "**Logistic Regression** and **Linear Regression** are both supervised learning algorithms used in machine learning and statistics, but they serve **different purposes** and are used for **different types of problems**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 What is **Logistic Regression**?\n",
        "\n",
        "**Logistic Regression** is used for **classification problems**, especially **binary classification** (e.g., yes/no, spam/not spam, 0/1).\n",
        "\n",
        "* It predicts the **probability** that a given input belongs to a particular class.\n",
        "* Instead of predicting a continuous number, it predicts a value between **0 and 1**, which represents a probability.\n",
        "* It uses the **sigmoid function** to squash the output of a linear equation into the range \\[0, 1].\n",
        "\n",
        "#### 🔸 Logistic Regression Formula:\n",
        "\n",
        "$$\n",
        "P(y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 What is **Linear Regression**?\n",
        "\n",
        "**Linear Regression** is used for **regression problems** (predicting continuous values like price, temperature, etc.).\n",
        "\n",
        "* It predicts a **real-valued output** based on the input features.\n",
        "* The goal is to find the best-fitting **straight line** through the data.\n",
        "\n",
        "#### 🔸 Linear Regression Formula:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Key Differences Between Logistic and Linear Regression:\n",
        "\n",
        "| Feature                   | **Linear Regression**                                 | **Logistic Regression**                                         |\n",
        "| ------------------------- | ----------------------------------------------------- | --------------------------------------------------------------- |\n",
        "| **Type of problem**       | Regression                                            | Classification                                                  |\n",
        "| **Output**                | Continuous value (e.g., 55.2, 101.5)                  | Probability between 0 and 1                                     |\n",
        "| **Activation Function**   | None (identity function)                              | Sigmoid function                                                |\n",
        "| **Output Interpretation** | Direct numerical prediction                           | Probability used for class prediction                           |\n",
        "| **Linearity**             | Assumes linear relationship between inputs and output | Assumes linear relationship between inputs and log-odds (logit) |\n",
        "| **Use Case Example**      | Predicting house prices                               | Predicting if a customer will buy or not                        |\n",
        "\n",
        "Great questions! Let's go through each one step by step to build a strong understanding of **Logistic Regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "The mathematical equation of **Logistic Regression** is based on the **sigmoid (logistic)** function applied to a linear combination of input features.\n",
        "\n",
        "#### ✅ Equation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\frac{1}{1 + e^{-z}} \\quad \\text{where } z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\hat{y}$: predicted probability (between 0 and 1)\n",
        "* $\\beta_0$: bias (intercept)\n",
        "* $\\beta_1, \\dots, \\beta_n$: weights (coefficients)\n",
        "* $x_1, \\dots, x_n$: input features\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "#### 🔸 Because:\n",
        "\n",
        "* Logistic regression needs to **predict probabilities**, which must lie between **0 and 1**.\n",
        "* The **sigmoid function** (also called the logistic function) maps any real-valued number (from $-\\infty$ to $+\\infty$) into the **range (0, 1)**.\n",
        "\n",
        "#### ✅ Sigmoid Function:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "#### 📌 Intuition:\n",
        "\n",
        "* If $z$ is very large, sigmoid gives output close to **1**\n",
        "* If $z$ is very small (negative), sigmoid gives output close to **0**\n",
        "* If $z = 0$, sigmoid gives **0.5**\n",
        "\n",
        "Thus, it’s perfect for **binary classification**, where we decide:\n",
        "\n",
        "* Class 1 if $\\hat{y} > 0.5$\n",
        "* Class 0 if $\\hat{y} \\leq 0.5$\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "In logistic regression, we use the **log loss** or **binary cross-entropy** as the cost function, because the output is a probability.\n",
        "\n",
        "#### ✅ Cost Function:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $m$: number of training examples\n",
        "* $y^{(i)}$: actual label (0 or 1)\n",
        "* $\\hat{y}^{(i)}$: predicted probability for example $i$\n",
        "\n",
        "This function penalizes wrong predictions more when the model is **confident but wrong**, helping the model learn better decision boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "#### 🔸 **Regularization** is a technique to prevent **overfitting** by **penalizing large weights** in the model.\n",
        "\n",
        "When the model becomes too complex (i.e., fits the training data too closely), it may not generalize well to new data. Regularization controls this.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Types of Regularization in Logistic Regression:\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge):**\n",
        "\n",
        "* Adds a penalty proportional to the **square** of the weights:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso):**\n",
        "\n",
        "* Adds a penalty proportional to the **absolute value** of weights:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "$$\n",
        "\n",
        "#### 🔹 Why is it needed?\n",
        "\n",
        "* To **reduce overfitting**\n",
        "* To keep the model **simple and interpretable**\n",
        "* L1 can also help with **feature selection** (it can shrink some weights to zero)\n",
        "\n",
        "\n",
        "Excellent follow-up questions! These go deeper into **regularization** and **classification modeling**, which is key to mastering machine learning. Let’s answer each in turn:\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Difference Between Lasso, Ridge, and Elastic Net Regression**\n",
        "\n",
        "| Feature                    | **Ridge Regression (L2)**               | **Lasso Regression (L1)**                  | **Elastic Net**           |   |                   |           |                                 |\n",
        "| -------------------------- | --------------------------------------- | ------------------------------------------ | ------------------------- | - | ----------------- | --------- | ------------------------------- |\n",
        "| **Penalty Term**           | $\\lambda \\sum \\theta_j^2$               | ( \\lambda \\sum                             | \\theta\\_j                 | ) | ( \\lambda\\_1 \\sum | \\theta\\_j | + \\lambda\\_2 \\sum \\theta\\_j^2 ) |\n",
        "| **Effect on Coefficients** | Shrinks them but doesn’t make them zero | Shrinks and can make some exactly **zero** | Combines both behaviors   |   |                   |           |                                 |\n",
        "| **Feature Selection**      |  No (all features retained)            |\n",
        " Yes (some dropped to 0)                  |  Yes (partial selection) |   |                   |           |                                 |\n",
        "| **When to Use**            | Many correlated features                | Sparse models, some irrelevant features    | Mix of both above         |   |                   |           |                                 |\n",
        "\n",
        "---\n",
        "\n",
        "### **7. When Should We Use Elastic Net Instead of Lasso or Ridge?**\n",
        "\n",
        "Use **Elastic Net** when:\n",
        "\n",
        "* You have **many features**, and some are **correlated**.\n",
        "* You suspect **some features are important** and others are not.\n",
        "* **Lasso alone** is too aggressive (dropping too many features).\n",
        "* **Ridge alone** doesn’t provide feature selection.\n",
        "\n",
        "✅ **Elastic Net** is a balance between **Lasso’s sparsity** and **Ridge’s stability** — especially useful in **high-dimensional data (p > n)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is the Impact of the Regularization Parameter (λ) in Logistic Regression?**\n",
        "\n",
        "The **regularization parameter (λ)** controls the **strength of penalty** added to the cost function.\n",
        "\n",
        "| λ value     | Effect                                                            |\n",
        "| ----------- | ----------------------------------------------------------------- |\n",
        "| **λ = 0**   | No regularization → Risk of **overfitting**                       |\n",
        "| **Small λ** | Slight penalty → Model is flexible                                |\n",
        "| **Large λ** | Strong penalty → Coefficients shrink → Can cause **underfitting** |\n",
        "\n",
        "🔁 So, **tuning λ** is crucial (often via **cross-validation**).\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Key Assumptions of Logistic Regression**\n",
        "\n",
        "Even though it's used for classification, logistic regression has some important assumptions:\n",
        "\n",
        "| Assumption                               | Explanation                                                                                                |\n",
        "| ---------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
        "| **Linear relationship with log-odds**    | The predictors have a **linear relationship** with the **logit** (log-odds), not with the output directly. |\n",
        "| **No multicollinearity**                 | Predictors should not be highly correlated. If they are, regularization or PCA may help.                   |\n",
        "| **Independent observations**             | Each training example is independent of others.                                                            |\n",
        "| **Large sample size**                    | Helps ensure stable and reliable probability estimates.                                                    |\n",
        "| **Binary outcome (for binary logistic)** | Target should be 0/1 or easily transformable to such.                                                      |\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Alternatives to Logistic Regression for Classification Tasks**\n",
        "\n",
        "Here are popular alternatives (especially when logistic regression doesn't perform well):\n",
        "\n",
        "| Algorithm                                       | Strength                                             |\n",
        "| ----------------------------------------------- | ---------------------------------------------------- |\n",
        "| **Decision Trees**                              | Simple, interpretable, non-linear boundaries         |\n",
        "| **Random Forest**                               | Handles non-linearities, reduces overfitting         |\n",
        "| **Support Vector Machines (SVM)**               | Effective for high-dimensional data                  |\n",
        "| **k-Nearest Neighbors (KNN)**                   | Intuitive, non-parametric                            |\n",
        "| **Naive Bayes**                                 | Very fast, works well with text and categorical data |\n",
        "| **Gradient Boosting (e.g., XGBoost, LightGBM)** | Powerful, handles complex patterns well              |\n",
        "| **Neural Networks**                             | Good for large datasets and complex relationships    |\n",
        "\n",
        "|\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What are Classification Evaluation Metrics?**\n",
        "\n",
        "Used to evaluate how well a classification model performs.\n",
        "\n",
        "| Metric                   | Meaning                                                                                            |\n",
        "| ------------------------ | -------------------------------------------------------------------------------------------------- |\n",
        "| **Accuracy**             | $\\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$                                      |\n",
        "| **Precision**            | $\\frac{\\text{TP}}{\\text{TP + FP}}$ – how many predicted positives are actually positive            |\n",
        "| **Recall (Sensitivity)** | $\\frac{\\text{TP}}{\\text{TP + FN}}$ – how many actual positives were captured                       |\n",
        "| **F1 Score**             | Harmonic mean of precision and recall: $2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$ |\n",
        "| **ROC-AUC**              | Measures the ability to distinguish between classes                                                |\n",
        "| **Confusion Matrix**     | Table showing TP, FP, FN, TN                                                                       |\n",
        "\n",
        "✅ F1 is especially useful in **imbalanced datasets**.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. How Does Class Imbalance Affect Logistic Regression?**\n",
        "\n",
        "* **Accuracy becomes misleading**: e.g., predicting all \"0\" gives 90% accuracy in 90:10 ratio.\n",
        "* **Bias toward majority class**\n",
        "* **Poor recall/precision** for minority class\n",
        "\n",
        "#### 🔧 Solutions:\n",
        "\n",
        "* **Use F1 Score or AUC instead of Accuracy**\n",
        "* **Resampling techniques** (oversample minority, undersample majority)\n",
        "* **Class weights** (`class_weight='balanced'` in sklearn)\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "It’s the process of **finding the best hyperparameter values** (not learned from data) for better performance.\n",
        "\n",
        "#### Common hyperparameters:\n",
        "\n",
        "* `C`: inverse of regularization strength (smaller = more regularization)\n",
        "* `penalty`: `'l1'`, `'l2'`, `'elasticnet'`\n",
        "* `solver`: optimization algorithm\n",
        "\n",
        "#### ✅ Use:\n",
        "\n",
        "* **Grid Search** or **Random Search**\n",
        "* **Cross-validation** to validate performance\n",
        "\n",
        "---\n",
        "\n",
        "### **14. What are Different Solvers in Logistic Regression? Which One Should Be Used?**\n",
        "\n",
        "| Solver        | Suitable For                      | Supports           |\n",
        "| ------------- | --------------------------------- | ------------------ |\n",
        "| **liblinear** | Small datasets                    | L1, L2             |\n",
        "| **saga**      | Large datasets & sparse data      | L1, L2, elasticnet |\n",
        "| **lbfgs**     | Default, efficient for multiclass | L2                 |\n",
        "| **newton-cg** | Large datasets                    | L2                 |\n",
        "\n",
        "✅ **Best choice:**\n",
        "\n",
        "* **liblinear** for binary + small data\n",
        "* **saga** for large data + L1/ElasticNet\n",
        "* **lbfgs** for multiclass + medium-large data\n",
        "\n",
        "---\n",
        "\n",
        "### **15. How is Logistic Regression Extended for Multiclass Classification?**\n",
        "\n",
        "Two common ways:\n",
        "\n",
        "#### ✅ One-vs-Rest (OvR):\n",
        "\n",
        "* Train 1 classifier per class vs all others\n",
        "* Simple and interpretable\n",
        "* Default in `scikit-learn`\n",
        "\n",
        "#### ✅ Softmax (Multinomial):\n",
        "\n",
        "* All classes trained simultaneously\n",
        "* Uses **softmax function**\n",
        "* More accurate for mutually exclusive classes\n",
        "\n",
        "---\n",
        "\n",
        "### **16. Advantages and Disadvantages of Logistic Regression**\n",
        "\n",
        "| Advantages                 | Disadvantages                      |\n",
        "| -------------------------- | ---------------------------------- |\n",
        "| Simple, fast               | Assumes linearity in log-odds      |\n",
        "| Probabilistic output       | Poor with non-linear data          |\n",
        "| Works well with small data | Sensitive to outliers              |\n",
        "| Interpretable              | Needs feature scaling & clean data |\n",
        "\n",
        "---\n",
        "\n",
        "### **17. Use Cases of Logistic Regression**\n",
        "\n",
        "* **Email spam detection**\n",
        "* **Customer churn prediction**\n",
        "* **Credit scoring (loan default)**\n",
        "* **Disease diagnosis (e.g., cancer detection)**\n",
        "* **Fraud detection**\n",
        "* **Marketing response prediction**\n",
        "\n",
        "---\n",
        "\n",
        "### **18. Difference Between Softmax Regression and Logistic Regression**\n",
        "\n",
        "| Feature  | Logistic Regression    | Softmax Regression                        |\n",
        "| -------- | ---------------------- | ----------------------------------------- |\n",
        "| Used For | Binary classification  | Multiclass classification                 |\n",
        "| Output   | Probability of class 1 | Probability distribution over all classes |\n",
        "| Function | Sigmoid                | Softmax                                   |\n",
        "\n",
        "#### ✅ Softmax formula:\n",
        "\n",
        "$$\n",
        "P(y = k \\mid x) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **19. Choosing Between One-vs-Rest (OvR) and Softmax**\n",
        "\n",
        "| Criteria                   | Use OvR | Use Softmax |\n",
        "| -------------------------- | ------- | ----------- |\n",
        "| Simpler, faster            | yes       | no          |\n",
        "| Better multiclass accuracy | no      | yes          |\n",
        "| Classes not exclusive      | yes       | no          |\n",
        "| Classes mutually exclusive | no       | yes           |\n",
        "\n",
        "✅ Use **Softmax** for clean, mutually exclusive classes (e.g., digit recognition).\n",
        "\n",
        "---\n",
        "\n",
        "### **20. How Do We Interpret Coefficients in Logistic Regression?**\n",
        "\n",
        "Each coefficient $\\beta_j$ represents the **change in log-odds** of the outcome per unit increase in $x_j$, holding other variables constant.\n",
        "\n",
        "#### Log-odds to odds:\n",
        "\n",
        "$$\n",
        "\\text{Odds ratio} = e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "✅ Interpretation:\n",
        "\n",
        "* $\\beta_j > 0$: increasing $x_j$ increases odds of class 1\n",
        "* $\\beta_j < 0$: increasing $x_j$ decreases odds\n",
        "* $\\beta_j = 0$: no effect\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7aEtZ7zdRr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical**"
      ],
      "metadata": {
        "id": "_Agvc1HXgFK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q.21  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracyC\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target (0 or 1)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLz7fCtRgI0_",
        "outputId": "821ac589-6159-43fb-d9f1-d671bdf9ec2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 95.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q.22 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "#and print the model accuracy ?\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Standardize the features (important for L1 regularization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"L1-Regularized Logistic Regression Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTC8RYhWqx8_",
        "outputId": "49d4c479-99b3-44a9-c0dc-0ce4906013ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 97.37%\n"
          ]
        }
      ]
    }
  ]
}